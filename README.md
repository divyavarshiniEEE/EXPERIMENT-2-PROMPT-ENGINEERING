# EXP-2-PROMPT-ENGINEERING-

## Aim: 
Comparative Analysis of different types of Prompting patterns and explain with Various Test Scenarios

Experiment:
Test and compare how different pattern models respond to various prompts (broad or unstructured) versus basic prompts (clearer and more refined) across multiple scenarios. 
Analyze the quality, accuracy, and depth of the generated responses.


## Algorithm:
*Identify different prompt styles: Broad, Basic, Refined.

*Select multiple test scenarios:

  1.Automated Test Script Generation

  2.Bug Identification in Test Cases

  3.Performance Test Planning

*Apply each prompt type to the scenarios.

*Document and evaluate outputs based on clarity, accuracy, depth, and relevance.

*Compare the results and analyze which prompting style yields the best outcome.

## Output
<img width="1600" height="343" alt="image" src="https://github.com/user-attachments/assets/fd9600f9-5960-4b2a-9c9a-b071aed20bf1" />

<img width="1590" height="499" alt="image" src="https://github.com/user-attachments/assets/723dc3a4-0d87-4aae-bddc-0da61e70dc72" />

<img width="1572" height="430" alt="image" src="https://github.com/user-attachments/assets/82be795d-bb1d-40ed-af8e-1da15b75bffb" />


## Result
From the comparison, it is evident that:

1.Broad prompts → produce vague and generic results with minimal usability.

2.Basic prompts → give moderately useful results but lack completeness.

3.Refined prompts → generate highly detailed, accurate, and context-aware outputs, making them the most effective for software testing tasks.

Thus, precise and structured prompt design significantly enhances the quality and practicality of AI-assisted testing.
